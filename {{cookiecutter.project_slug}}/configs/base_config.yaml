dataset: null
trainer: null
evaluators: []

batch_size: 512
accumulation_steps: 1

epochs: 300
eval_every: 1


# loss_args:
  # temperature: 0.001

early_stopping:
  patience: 15
  monitor: val_loss

lr_scheduler:
  start_epoch: 0
  end_epoch: 300
  base_lr: 0.0001
  max_lr:  0.001
  mode: triangular
  step_size_up: 10
  step_size_down: 10

model: transformer
model_args:
  embedding_size: null
